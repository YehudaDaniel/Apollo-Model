{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apollo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pretty_midi\n",
    "from keras import layers, Model, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMT():\n",
    "    def __init__(self, config, model_path, batch_size = 1, verbose = False):\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        if model_path is None:\n",
    "            self.model = None\n",
    "\n",
    "        else:\n",
    "            with open(model_path, 'rb') as model_file:\n",
    "                self.model = pickle.load(model_file) #Read a binary file\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            if verbose:\n",
    "                print(self.model)\n",
    "            \n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    \n",
    "    def wavToFeatures(self, wav_file):\n",
    "        y_mono, sr = librosa.load(wav_file, mono = True)\n",
    "\n",
    "        # Resample\n",
    "        target_sr = self.config['feature']['sr']\n",
    "        y_resampled = librosa.resample(y_mono, orig_sr= sr, target_sr = target_sr)\n",
    "\n",
    "        # Compute Mel Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y= y_resampled,\n",
    "            sr = target_sr,\n",
    "            n_fft = self.config['feature']['fft_bins'],\n",
    "            hop_length = self.config['feature']['hop_sample'],\n",
    "            n_mels = self.config['feature']['mel_bins'],\n",
    "            window = 'hann')\n",
    "        \n",
    "        # Convert to log scale\n",
    "        log_offset = self.config['feature']['log_offset']\n",
    "        mel_spectrogram_log = np.log(mel_spectrogram + log_offset)\n",
    "\n",
    "        # Trasnpose the spectrogram\n",
    "        mel_spectrogram_log_transposed = mel_spectrogram_log.T\n",
    "\n",
    "        # Convert the spectrogram to Tensor\n",
    "        a_feature = tf.convert_to_tensor(mel_spectrogram_log_transposed, dtype=tf.float32)\n",
    "\n",
    "        return a_feature\n",
    "    \n",
    "\n",
    "    def transcript(self, a_feature, mode = 'combination', ablation = False):\n",
    "        a_feature = np.array(a_feature, dtype=tf.float32)\n",
    "\n",
    "        # Create the padding at the beginning (before the feature)\n",
    "        a_tmp_b = np.full(\n",
    "            [self.config[input]['margin_b'], self.config['feature']['n_bins']],\n",
    "            self.config['input']['min_value'],\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        # Calculate the length of padding needed at the end\n",
    "        len_s = int(np.ceil(a_feature[0] / self.config['input']['num_frame'])* self.config['input']['num_frame']) - a_feature.shape[0]\n",
    "\n",
    "        # Create the padding at the end (after the feature)\n",
    "        a_tmp_f = np.full(\n",
    "            [len_s + self.config['input']['margin_f'], self.config['feature']['n_bins']],\n",
    "            self.config['input']['min_value'],\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        # Concatenate the beginning padding, feature, and finish padding\n",
    "        a_input = np.concatenate([a_tmp_b, a_feature.numpy(), a_tmp_f], axis=0)\n",
    "\n",
    "        # Convert to a TesnsorFlow tensor\n",
    "        a_input = tf.convert_to_tensor(a_input, dtype=tf.float32)\n",
    "\n",
    "        a_output_onset_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.congif['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        a_output_offset_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        a_output_mpe_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        a_output_velocity_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        if mode == 'combination':\n",
    "            a_output_onset_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "            a_output_offset_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "            a_output_mpe_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "            a_output_velocity_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        for i in range(0, a_feature.shape[0], self.config['input']['num_frame']):\n",
    "            input_spec = tf.transpose(a_input[i:i+self.config['input']['margin_b']+self.config['input']['num_frame']+self.config['input']['margin_f']], perm=[1, 0])\n",
    "            input_spec = tf.expand_dims(input_spec, axis = 0)\n",
    "\n",
    "            if mode == 'combination':\n",
    "                if ablation:\n",
    "                    output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, \\\n",
    "                    output_onset_B, output_offset_B, output_mpe_B, output_velocity_B = self.model(input_spec, training=False)\n",
    "                else:\n",
    "                    output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, attention, \\\n",
    "                    output_onset_B, output_offset_B, output_mpe_B, output_velocity_B = self.model(input_spec, training=False)\n",
    "            else:\n",
    "                output_onset_A, output_offset_A, output_mpe_A, output_velocity_A = self.model(input_spec, training=False)\n",
    "            \n",
    "            a_output_onset_A[i:i+self.config['input']['num_frame']] = tf.squeeze(output_onset_A, axis=0).numpy()\n",
    "            a_output_offset_A[i:i+self.config['input']['num_frame']] = tf.squeeze(output_offset_A, axis=0).numpy()\n",
    "            a_output_mpe_A[i:i+self.config['input']['num_frame']] = tf.squeeze(output_mpe_A, axis=0).numpy()\n",
    "            a_output_velocity_A[i:i+self.config['input']['num_frame']] = tf.argmax(tf.squeeze(output_velocity_A, axis=0), axis=2).numpy()\n",
    "\n",
    "            if mode == 'combination':\n",
    "                a_output_onset_B[i:i+self.config['input']['num_frame']] = tf.squeeze(output_onset_B, axis=0).numpy()\n",
    "                a_output_offset_B[i:i+self.config['input']['num_frame']] = tf.squeeze(output_offset_B, axis=0).numpy()\n",
    "                a_output_mpe_B[i:i+self.config['input']['num_frame']] = tf.squeeze(output_mpe_B, axis=0).numpy()\n",
    "                a_output_velocity_B[i:i+self.config['input']['num_frame']] = tf.argmax(tf.squeeze(output_velocity_B, axis=0), axis=2).numpy()\n",
    "\n",
    "        if mode == 'combination':\n",
    "            return a_output_onset_A, a_output_offset_A, a_output_mpe_A, a_output_velocity_A, \\\n",
    "                   a_output_onset_B, a_output_offset_B, a_output_mpe_B, a_output_velocity_B\n",
    "        else:\n",
    "            return a_output_onset_A, a_output_offset_A, a_output_mpe_A, a_output_velocity_A\n",
    "    \n",
    "    def trasnscipt_stride(self, a_feature, n_offset, mode = 'combination', ablation = False):\n",
    "        a_feature = np.array(a_feature, dtype=tf.float32)\n",
    "\n",
    "        # Create the padding at the beginning (before the feature)\n",
    "        half_frame = int(self.config['input']['num_frame']/2)\n",
    "        a_tmp_b = np.full(\n",
    "            [self.config[input]['margin_b']+ n_offset, self.config['feature']['n_bins']],\n",
    "            self.config['input']['min_value'],\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        # Calculate the length of padding needed at the end\n",
    "        tmp_len = a_feature.shape[0] + self.config['input']['margin_b'] + self.config['input']['margin_f'] + half_frame\n",
    "        len_s = int(np.ceil(tmp_len / half_frame) * half_frame) - tmp_len\n",
    "\n",
    "        # Create the padding at the end (after the feature)\n",
    "        a_tmp_f = np.full(\n",
    "            [len_s+self.config['input']['margin_f']+ (half_frame-n_offset), self.config['feature']['n_bins']],\n",
    "            self.config['input']['min_value'],\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        # Concatenate the beginning padding, feature, and end padding\n",
    "        a_input = np.concatenate([a_tmp_b, a_feature.numpy(), a_tmp_f], axis=0)\n",
    "\n",
    "        # Convert to a TesnsorFlow tensor\n",
    "        a_input = tf.convert_to_tensor(a_input, dtype=tf.float32)\n",
    "\n",
    "        a_output_onset_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.congif['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        a_output_offset_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        a_output_mpe_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        a_output_velocity_A = np.zeros(\n",
    "            (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "            dtype= np.float32\n",
    "        )\n",
    "\n",
    "        if mode == 'combination':\n",
    "            a_output_onset_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "            a_output_offset_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "            a_output_mpe_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "            a_output_velocity_B = np.zeros(\n",
    "                (a_feature.shape[0] + len_s, self.config['midi']['num_note']),\n",
    "                dtype= np.float32\n",
    "            )\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        for i in range(0, a_feature.shape[0], half_frame):\n",
    "            input_spec = tf.transpose(a_input[i:i+self.config['input']['margin_b']+self.config['input']['num_frame']+self.config['input']['margin_f']], perm=[1, 0])\n",
    "            input_spec = tf.expand_dims(input_spec, axis = 0)\n",
    "\n",
    "            if mode == 'combination':\n",
    "                if ablation:\n",
    "                    output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, \\\n",
    "                    output_onset_B, output_offset_B, output_mpe_B, output_velocity_B = self.model(input_spec, training=False)\n",
    "                else:\n",
    "                    output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, attention, \\\n",
    "                    output_onset_B, output_offset_B, output_mpe_B, output_velocity_B = self.model(input_spec, training=False)\n",
    "            else:\n",
    "                output_onset_A, output_offset_A, output_mpe_A, output_velocity_A = self.model(input_spec, training=False)\n",
    "            \n",
    "            a_output_onset_A[i:i+half_frame] = tf.squeeze(output_onset_A, axis=0)[n_offset:n_offset+half_frame].numpy()\n",
    "            a_output_offset_A[i:i+half_frame] = tf.squeeze(output_offset_A, axis=0)[n_offset:n_offset+half_frame].numpy()\n",
    "            a_output_mpe_A[i:i+half_frame] = tf.squeeze(output_mpe_A, axis=0)[n_offset:n_offset+half_frame].numpy()\n",
    "            a_output_velocity_A[i:i+half_frame] = tf.argmax(tf.squeeze(output_velocity_A, axis=0)[n_offset:n_offset+half_frame], axis=2).numpy()\n",
    "\n",
    "            if mode == 'combination':\n",
    "                a_output_onset_B[i:i+half_frame] = tf.squeeze(output_onset_B, axis=0)[n_offset:n_offset+half_frame].numpy()\n",
    "                a_output_offset_B[i:i+half_frame] = tf.squeeze(output_offset_B, axis=0)[n_offset:n_offset+half_frame].numpy()\n",
    "                a_output_mpe_B[i:i+half_frame] = tf.squeeze(output_mpe_B, axis=0)[n_offset:n_offset+half_frame].numpy()\n",
    "                a_output_velocity_B[i:i+half_frame] = tf.argmax(tf.squeeze(output_velocity_B, axis=0)[n_offset:n_offset+half_frame], axis=2).numpy()\n",
    "\n",
    "        if mode == 'combination':\n",
    "            return a_output_onset_A, a_output_offset_A, a_output_mpe_A, a_output_velocity_A, \\\n",
    "                   a_output_onset_B, a_output_offset_B, a_output_mpe_B, a_output_velocity_B\n",
    "        else:\n",
    "            return a_output_onset_A, a_output_offset_A, a_output_mpe_A, a_output_velocity_A\n",
    "    \n",
    "    def mpeToNote(self, a_onset = None, a_offset = None, a_mpe = None, a_velocity = None, thred_onset = 0.5, thred_offset = 0.5, thred_mpe = 0.5, mode_velocity = 'ignore_zero', mode_offset = 'shorter'):\n",
    "        a_note = []\n",
    "        hop_sec = float(self.congif['feature']['hop_sample'] / self.config['feature']['sr'])\n",
    "        \n",
    "        for j in range(self.config['midi']['num_note']):\n",
    "            a_onset_detect = []\n",
    "\n",
    "            for i in range(len(a_onset)):\n",
    "                if a_onset[i][j] >= thred_onset:\n",
    "                    left_flag = True\n",
    "                    for k in range (i-1, -1, -1):\n",
    "                        if a_onset[i][j] > a_onset[k][j]:\n",
    "                            left_flag = True\n",
    "                            break\n",
    "                        elif a_onset[i][j] < a_onset[k][j]:\n",
    "                            left_flag = False\n",
    "                            break\n",
    "                    right_flag = True\n",
    "                    for k in range(i+1, len(a_onset)):\n",
    "                        if a_onset[i][j] > a_onset[k][j]:\n",
    "                            right_flag = True\n",
    "                            break\n",
    "                        elif a_onset[i][j] < a_onset[k][j]:\n",
    "                            right_flag = False\n",
    "                            break\n",
    "                    if left_flag and right_flag:\n",
    "                        if i==0 or i == len(a_onset)-1:\n",
    "                            onset_time = i* hop_sec\n",
    "                        else:\n",
    "                            if a_onset[i-1][j] == a_onset[i+1][j]:\n",
    "                                onset_time = i * hop_sec\n",
    "                            elif a_onset[i-1][j] > a_onset[i+1][j]:\n",
    "                                    onset_time = (i * hop_sec - (hop_sec * 0.5 * (a_onset[i-1][j] - a_onset[i+1][j]) / (a_onset[i][j] - a_onset[i+1][j])))\n",
    "                            else:\n",
    "                                onset_time = (i * hop_sec + (hop_sec * 0.5 * (a_onset[i+1][j] - a_onset[i-1][j]) / (a_onset[i][j] - a_onset[i-1][j])))\n",
    "                        a_onset_detect.append({'loc': i, 'onset_time': onset_time})\n",
    "                \n",
    "                a_offset_detect = []\n",
    "\n",
    "                for i in range(len(a_offset)):\n",
    "                    if a_offset[i][j] >= thred_offset:\n",
    "                        left_flag = True\n",
    "                        for k in range (i-1, -1, -1):\n",
    "                            if a_offset[i][j] > a_offset[k][j]:\n",
    "                                left_flag = True\n",
    "                                break\n",
    "                            elif a_offset[i][j] < a_offset[k][j]:\n",
    "                                left_flag = False\n",
    "                                break\n",
    "                        right_flag = True\n",
    "                        for k in range(i+1, len(a_offset)):\n",
    "                            if a_offset[i][j] > a_offset[k][j]:\n",
    "                                right_flag = True\n",
    "                                break\n",
    "                            elif a_offset[i][j] < a_offset[k][j]:\n",
    "                                right_flag = False\n",
    "                                break\n",
    "                        if left_flag and right_flag:\n",
    "                            if i==0 or i == len(a_offset)-1:\n",
    "                                offset_time = i* hop_sec\n",
    "                            else:\n",
    "                                if a_offset[i-1][j] == a_offset[i+1][j]:\n",
    "                                    offset_time = i * hop_sec\n",
    "                                elif a_offset[i-1][j] > a_offset[i+1][j]:\n",
    "                                    offset_time = (i * hop_sec - (hop_sec * 0.5 * (a_offset[i-1][j] - a_offset[i+1][j]) / (a_offset[i][j] - a_offset[i+1][j])))\n",
    "                                else:\n",
    "                                    offset_time = (i * hop_sec + (hop_sec * 0.5 * (a_offset[i+1][j] - a_offset[i-1][j]) / (a_offset[i][j] - a_offset[i-1][j])))\n",
    "                            a_offset_detect.append({'loc': i, 'offset_time': offset_time})\n",
    "                \n",
    "                time_next = 0.0\n",
    "                time_offset = 0.0\n",
    "                time_mpe = 0.0\n",
    "\n",
    "                for idx_on in range(len(a_onset_detect)):\n",
    "                    loc_onset = a_onset_detect[idx_on]['loc']\n",
    "                    time_onset = a_onset_detect[idx_on]['onset_time']\n",
    "\n",
    "                    if idx_on + 1 < len(a_onset_detect):\n",
    "                        loc_next = a_onset_detect[idx_on+1]['loc']\n",
    "                        time_next = a_onset_detect[idx_on+1]['onset_time']\n",
    "                    else:\n",
    "                        loc_next = len(a_mpe) \n",
    "                        time_next = (loc_next - 1) * hop_sec\n",
    "                    \n",
    "                    # offset\n",
    "                    loc_offset = loc_onset + 1\n",
    "                    flag_offset = False\n",
    "\n",
    "                    for idx_off in range(len(a_offset_detect)):\n",
    "                        if loc_onset < a_offset_detect[idx_off]['loc']:\n",
    "                            loc_offset = a_offset_detect[idx_off]['loc']\n",
    "                            time_offset = a_offset_detect[idx_off]['offset_time']\n",
    "                            flag_offset = True\n",
    "                            break\n",
    "                        if loc_offset > loc_next:\n",
    "                            loc_offset = loc_next\n",
    "                            time_offset = time_next\n",
    "                    \n",
    "                    loc_mpe = loc_onset + 1\n",
    "                    flag_mpe = False\n",
    "\n",
    "                    for idx_mpe in range(len(a_mpe)):\n",
    "                        if a_mpe[idx_mpe][j] < thred_mpe:\n",
    "                            loc_mpe = idx_mpe\n",
    "                            time_mpe = loc_mpe * hop_sec\n",
    "                            flag_mpe = True\n",
    "                            break\n",
    "                    \n",
    "\n",
    "                    pitch_value = int(j + self.config['midi']['note_min'])\n",
    "                    velocity_value = int(a_velocity[loc_onset][j])\n",
    "\n",
    "                    if flag_offset is False and flag_mpe is False:\n",
    "                        offset_value = float(time_next)\n",
    "                    elif flag_offset is True and flag_mpe is False:\n",
    "                        offset_value = float(time_offset)\n",
    "                    elif flag_offset is False and flag_mpe is True:\n",
    "                        offset_value = float(time_mpe)\n",
    "                    else:\n",
    "                        if mode_offset == 'offset':\n",
    "                            offset_value = float(time_offset)\n",
    "                        elif mode_offset == 'longer':\n",
    "                            if loc_offset >= loc_mpe:\n",
    "                                offset_value = float(time_offset)\n",
    "                            else:\n",
    "                                offset_value = float(time_mpe)\n",
    "                        else:\n",
    "                            if loc_offset <= loc_mpe:\n",
    "                                offset_value = float(time_offset)\n",
    "                            else:   \n",
    "                                 offset_value = float(time_mpe)\n",
    "                    \n",
    "                    if mode_velocity != 'ignore_zero':\n",
    "                        a_note.append({'pitch': pitch_value, 'onset': float(time_onset), 'offset': offset_value, 'velocity': velocity_value})\n",
    "                    else:\n",
    "                        if velocity_value > 0:\n",
    "                            a_note.append({'pitch': pitch_value, 'onset': float(time_onset), 'offset': offset_value, 'velocity': velocity_value})\n",
    "                    \n",
    "                    if (len(a_note) > 0) and (a_note[len(a_note)-1]['pitch'] == a_note[len(a_note)-2]['pitch']) and (a_note[len(a_note)-1]['onset'] == a_note[len(a_note)-2]['onset']):\n",
    "                        a_note[len(a_note)-2]['offset'] = a_note[len(a_note)-1]['onset']\n",
    "                    \n",
    "        a_note = sorted(sorted(a_note, key = lambda x : x['pitch']), key = lambda x : x['onset'])\n",
    "        return a_note\n",
    "\n",
    "    def noteToMIDI(self, a_note, midi_file):\n",
    "        midi = pretty_midi.PrettyMIDI()\n",
    "        instrument = pretty_midi.Instrument(program=0)\n",
    "        for note in a_note:\n",
    "            instrument.notes.append(pretty_midi.Note(velocity=note['velocity'], pitch=note['pitch'], start=note['onset'], end=note['offset']))\n",
    "        midi.instruments.append(instrument)\n",
    "        midi.write(midi_file)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model spectogram to midi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_SPEC2MIDI(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder_spec2midi = encoder\n",
    "        self.decoder_spec2midi = decoder\n",
    "\n",
    "    def forward(self, input_spec):\n",
    "        enc_vector = self.encoder_spec2midi(input_spec)\n",
    "        \n",
    "        output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, attention, output_onset_B, output_offset_B, output_mpe_B, output_velocity_B = self.decoder_spec2midi(enc_vector)\n",
    "\n",
    "        return output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, output_onset_B, output_offset_B, output_mpe_B, output_velocity_B, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_SPEC2MIDI(Model):\n",
    "    def __init__(self, n_margin,n_frame, n_bin, cnn_channel, cnn_kernel, hid_dim, n_layers, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_frame = n_frame\n",
    "        self.n_bin = n_bin\n",
    "        self.cnn_channel = cnn_channel\n",
    "        self.cnn_kernel = cnn_kernel\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.conv = layers.Conv2D(1, self.cnn_channel, kernel_size=(1, self.cnn_kernel))\n",
    "        self.n_proc = n_margin * 2 + 1\n",
    "        self.cnn_dim = self.cnn_channel * (self.n_proc - (self.cnn_kernel - 1))\n",
    "        self.tok_embedding_freq = layers.Dense(hid_dim, self.cnn_dim)\n",
    "        self.pos_embedding_freq = layers.Embedding(n_bin, hid_dim)\n",
    "        self.layers_freq = [\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.scale_freq = tf.sqrt(tf.constant([hid_dim], dtype= tf.float32))\n",
    "\n",
    "        with tf.device('/gpu:0'):\n",
    "           self.scale_freq = tf.sqrt(tf.constant([hid_dim], dtype= tf.float32))\n",
    "\n",
    "    def forward(self, spec_in):\n",
    "        batch_size = spec_in.shape[0]\n",
    "\n",
    "        spec = tf.image.extract_patches(\n",
    "            images = tf.expand_dims(spec_in, -1),\n",
    "            sizes = [1, 1, self.n_proc, 1],\n",
    "            strides = [1, 1, 1, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "\n",
    "        spec = tf.reshape(spec, (batch_size, self.n_frame, self.n_bin, self.n_proc))\n",
    "\n",
    "        spec_cnn = tf.reshape(spec, (batch_size * self.n_frame, self.n_bin, self.n_proc, 1))\n",
    "        spec_cnn = self.conv(spec_cnn)\n",
    "        spec_cnn = tf.transpose(spec_cnn, perm=[0, 2, 3, 1])\n",
    "        spec_cnn = tf.reshape(spec_cnn, (batch_size * self.n_frame, self.n_bin, self.cnn_dim))\n",
    "\n",
    "        spec_cnn_freq = spec_cnn\n",
    "        spec_emb_freq = self.tok_embedding_freq(spec_cnn_freq)\n",
    "\n",
    "        pos_freq = tf.range(0, self.n_bin)\n",
    "        pos_freq = tf.tile(tf.expand_dims(pos_freq,0), [batch_size * self.n_frame, 1])\n",
    "        spec_freq = self.dropout((spec_emb_freq * self.scale_freq) + self.pos_embedding_freq(pos_freq))\n",
    "\n",
    "        for layer_freq in self.layers_freq:\n",
    "            spec_freq = layer_freq(spec_freq)\n",
    "        \n",
    "\n",
    "        spec_freq = tf.reshape(spec_freq, (batch_size, self.n_frame, self.n_bin, self.hid_dim))\n",
    "\n",
    "        return spec_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_SPEC2MIDI(Model):\n",
    "    def __init__(self,n_frame, n_bin, n_note, n_velocity, hid_dim, n_layers, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_frame = n_frame\n",
    "        self.n_bin = n_bin\n",
    "        self.n_note = n_note\n",
    "        self.n_velocity = n_velocity\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.sigmoid = layers.Activation('sigmoid')\n",
    "        self.dropout = layers.Dropout(rate = dropout)\n",
    "\n",
    "        self.pos_embedding_freq = layers.Embedding(n_note, hid_dim)\n",
    "        self.layer_zero_freq = DecoderLayer_Zero(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "        self.layers_freq = [\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "            for _ in range(n_layers - 1)\n",
    "        ]\n",
    "\n",
    "        self.fc_onset_freq = layers.Dense(hid_dim, 1)\n",
    "        self.fc_offset_freq = layers.Dense(hid_dim, 1)\n",
    "        self.fc_mpe_freq = layers.Dense(hid_dim, 1)\n",
    "        self.fc_velocity_freq = layers.Dense(hid_dim, self.n_velocity)\n",
    "\n",
    "        self.scale_freq = tf.sqrt(tf.constant([hid_dim], dtype= tf.float32))\n",
    "\n",
    "        self.pos_embedding_freq = layers.Embedding(n_frame, hid_dim)\n",
    "        self.layers_time = [\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.fc_onset_time = layers.Dense(hid_dim, 1)\n",
    "        self.fc_offset_time = layers.Dense(hid_dim, 1)\n",
    "        self.fc_mpe_time = layers.Dense(hid_dim, 1)\n",
    "        self.fc_velocity_time = layers.Dense(hid_dim, self.n_velocity)\n",
    "\n",
    "    def forward(self, enc_spec):\n",
    "        batch_size = enc_spec.shape[0]\n",
    "        enc_spec = enc_spec.reshape([batch_size * self.n_frame, self.n_bin, self.hid_dim])\n",
    "\n",
    "        pos_freq = tf.range(self.n_note, dtype=tf.float32)[tf.newaxis, :]  # Create a 1D tensor and add a new axis\n",
    "        pos_freq = tf.repeat(pos_freq, repeats=batch_size * self.n_frame, axis=0)  # Repeat the row\n",
    "\n",
    "        midi_freq = self.pos_embedding_freq(pos_freq)\n",
    "\n",
    "        midi_freq, attention_freq = self.layer_zero_freq(enc_spec, midi_freq)\n",
    "\n",
    "        for layer_freq in self.layers_freq:\n",
    "            midi_freq, attention_freq = layer_freq(enc_spec, midi_freq)\n",
    "\n",
    "        dim = attention_freq.shape\n",
    "        attention_freq = attention_freq.reshape([batch_size, self.n_frame, dim[1], dim[2], dim[3]])\n",
    "\n",
    "        output_onset_freq = self.sigmoid(self.fc_onset_freq(midi_freq).reshape([batch_size, self.n_frame, self.n_note]))\n",
    "        output_offset_freq = self.sigmoid(self.fc_offset_freq(midi_freq).reshape([batch_size, self.n_frame, self.n_note]))\n",
    "        output_mpe_freq = self.sigmoid(self.fc_mpe_freq(midi_freq).reshape([batch_size, self.n_frame, self.n_note]))\n",
    "        output_velocity_freq = self.fc_velocity_freq(midi_freq).reshape([batch_size, self.n_frame, self.n_note, self.n_velocity])\n",
    "\n",
    "        midi_time = midi_freq.reshape([batch_size, self.n_frame, self.n_note, self.hid_dim]).permute(0, 2, 1, 3).contiguous().reshape([batch_size*self.n_note, self.n_frame, self.hid_dim])\n",
    "\n",
    "        pos_time = tf.range(self.n_frame, dtype=tf.float32)[tf.newaxis, :]  # Create a 1D tensor and add a new axis\n",
    "        pos_time = tf.repeat(pos_time, repeats=batch_size * self.n_note, axis=0)  # Repeat the row        \n",
    "        midi_time = self.dropout((midi_time * self.scale_time) + self.pos_embedding_time(pos_time))\n",
    "\n",
    "        for layer_time in self.layers_time:\n",
    "            midi_time, _ = layer_time(midi_time)\n",
    "\n",
    "        output_onset_time = self.sigmoid(self.fc_onset_time(midi_time).reshape([batch_size, self.n_note, self.n_frame]).permute(0, 2, 1).contiguous())\n",
    "        output_offset_time = self.sigmoid(self.fc_offset_time(midi_time).reshape([batch_size, self.n_note, self.n_frame]).permute(0, 2, 1).contiguous())\n",
    "        output_mpe_time = self.sigmoid(self.fc_mpe_time(midi_time).reshape([batch_size, self.n_note, self.n_frame]).permute(0, 2, 1).contiguous())\n",
    "        output_velocity_time = self.fc_velocity_time(midi_time).reshape([batch_size, self.n_note, self.n_frame, self.n_velocity]).permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        return output_onset_freq, output_offset_freq, output_mpe_freq, output_velocity_freq, attention_freq, output_onset_time, output_offset_time, output_mpe_time, output_velocity_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForwardLayer(hid_dim, pf_dim, dropout)\n",
    "\n",
    "        self.dropout = layers.Dropout(rate = dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        _src, _ = self.self_attention(src, src, src)\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "        _src = self.feed_forward(src)\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src #recieve data vector of the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Layer Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer_Zero(layers):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feed_forward = PositionwiseFeedForwardLayer(hid_dim, pf_dim, dropout)\n",
    "\n",
    "        self.dropout = layers.Dropout(rate = dropout)\n",
    "\n",
    "    def forward(self, enc_src, trg):\n",
    "\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src)\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "        _trg = self.feed_forward(trg)\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention #recieve a vector for initializing the SPEC2MIDI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.layer_norm = layers.LayerNormalization(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feed_forward = PositionwiseFeedForwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def forward(self, enc_src, trg):\n",
    "        #trg = [batch_size, trg_len, hid_dim]\n",
    "        #enc_src = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg)\n",
    "        #dropout, residual layer and layer normalization\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        #encoder attention layer\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src)\n",
    "        #dropout, residual and layer normalization\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        #feed forward\n",
    "        _trg = self.feed_forward(trg)\n",
    "        #dropout, residual and layer normalization\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiHead Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(layers):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.fc_q = layers.Dense(hid_dim, hid_dim)\n",
    "        self.fc_k = layers.Dense(hid_dim, hid_dim)\n",
    "        self.fc_v = layers.Dense(hid_dim, hid_dim)\n",
    "        self.fc_o = layers.Dense(hid_dim, hid_dim)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.scale = tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "\n",
    "        def forward(self, query, key, value):\n",
    "            batch_size = query.shape[0]\n",
    "            #query = [batch_size, query_len, hid_dim]\n",
    "            #key = [batch_size, key_len, hid_dim]\n",
    "            #value = [batch_size, value_len, hid_dim]\n",
    "\n",
    "            Q = self.fc_q(query)\n",
    "            K = self.fc_k(query)\n",
    "            V = self.fc_v(query)\n",
    "\n",
    "            Q = tf.reshape(Q, [batch_size, -1, self.n_heads, self.head_dim])\n",
    "            Q = tf.transpose(Q, perm=[0, 2, 1, 3])\n",
    "\n",
    "            K = tf.reshape(K, [batch_size, -1, self.n_heads, self.head_dim])\n",
    "            K = tf.transpose(K, perm=[0, 2, 1, 3])\n",
    "\n",
    "            V = tf.reshape(V, [batch_size, -1, self.n_heads, self.head_dim])\n",
    "            V = tf.transpose(V, perm=[0, 2, 1, 3])\n",
    "\n",
    "            energy = tf.matmul(Q, tf.transpose(K, perm = [0, 1, 3, 2])) / self.scale\n",
    "\n",
    "            attention = activations.softmax(energy, axis=-1)\n",
    "\n",
    "            x = tf.matmul(self.dropout(attention), V)\n",
    "\n",
    "            x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "            x = tf.reshape(x, [batch_size, -1, self.hid_dim])\n",
    "\n",
    "            x = self.fc_o(x)\n",
    "\n",
    "            return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positionwise Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardLayer(layers):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = layers.Dense(hid_dim, pf_dim)\n",
    "        self.fc_2 = layers.Dense(pf_dim, hid_dim)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = [batch_size, seq_len, hid_dim]\n",
    "\n",
    "        x = self.dropout(activations.relu(self.fc_1(x)))\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectogram to MIDI Ablation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_single(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, input_spec):\n",
    "        # input spec = [batch_size, n_bin, margin + n_frame + margin], (8, 256, 192)\n",
    "\n",
    "        enc_vector = self.encoder_spec2midi(input_spec)\n",
    "\n",
    "        output_onset, output_offset, output_mpe, output_velocity = self.decoder_spec2midi(enc_vector)\n",
    "\n",
    "        return output_onset, output_offset, output_mpe, output_velocity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_combination(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_spec):\n",
    "        # input spec = [batch_size, n_bin, margin + n_frame + margin], (8, 256, 192)\n",
    "\n",
    "        enc_vector = self.encoder_spec2midi(input_spec)\n",
    "\n",
    "        output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, output_onset_B, output_offset_B, output_mpe_B, output_velocity_B = self.decoder_spec2midi(enc_vector)\n",
    "\n",
    "        return output_onset_A, output_offset_A, output_mpe_A, output_velocity_A, output_onset_B, output_offset_B, output_mpe_B, output_velocity_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_CNNtime_SAfreq(tf.Module):\n",
    "    def __init__(self, n_margin, n_frame, n_bin, cnn_channel, cnn_kernel, hid_dim, n_layers, n_heads, pf_dim, dropout, device):\n",
    "        super.__init__()\n",
    "\n",
    "        self.n_frame = n_frame\n",
    "        self.n_bin = n_bin\n",
    "        self.cnn_channel = cnn_channel\n",
    "        self.cnn_kernel = cnn_kernel\n",
    "        self.hid_dim = hid_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.conv = layers.Conv2D(1, self.cnn_channel, (1, self.cnn_kernel))\n",
    "        self.n_proc = n_margin * 2 + 1 \n",
    "        self.cnn_dim = self.cnn_channel * (self.n_proc - (self.cnn_kernel - 1))\n",
    "        self.tok_embedding_freq = layers.Dense(self.cnn_dim, hid_dim)\n",
    "        self.pos_embedding_freq = layers.Embedding(self.n_bin, self.hid_dim)\n",
    "\n",
    "        self.layers_freq = [\n",
    "            EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.scale_freq = tf.math.sqrt(tf.cast(self.hid_dim, tf.float32))\n",
    "\n",
    "\n",
    "    def forward(self, spec_in):\n",
    "        #spec_in = [batch_size, cnn_channel, n_bin, n_margin + n_frame + n_margin - (cnn_kernel - 1)] (8, 4, 256, 188)\n",
    "\n",
    "        batch_size = spec_in.shape[0]\n",
    "\n",
    "        spec_cnn = self.conv(tf.expand_dims(spec_in, axis=1))\n",
    "\n",
    "        spec_cnn = tf.image.extract_patches(\n",
    "            images=spec_cnn,\n",
    "            sizes=[1, 1, 61, 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        # Equivalent to PyTorch permute\n",
    "        spec_cnn = tf.transpose(spec_cnn, perm=[0, 3, 2, 1, 4])\n",
    "\n",
    "        # Equivalent to PyTorch reshape\n",
    "        spec_cnn = tf.reshape(spec_cnn, [batch_size * self.n_frame, self.n_bin, self.cnn_dim])\n",
    "\n",
    "        spec_emb_freq = self.tok_embedding_freq(spec_cnn)\n",
    "\n",
    "        pos_freq = tf.tile(tf.expand_dims(tf.range(self.n_bin), 0), [batch_size * self.n_frame, 1])\n",
    "        pos_freq = tf.cast(pos_freq, tf.float32)\n",
    "\n",
    "        spec_freq = self.dropout((spec_emb_freq * self.scale_freq) + self.pos_embbedding_freq(pos_freq))\n",
    "\n",
    "        for layer_freq in self.layers_freq:\n",
    "            spec_freq = layer_freq(spec_freq)\n",
    "        \n",
    "        spec_freq = tf.reshape(spec_freq, [batch_size, self.n_frame, self.n_bin, self.hid_dim])\n",
    "\n",
    "        return spec_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder CNN block + SA freq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_CNNblock_SAfreq(tf.Module):\n",
    "    def __init__(self, n_margin, n_frame, n_bin, hid_dim, n_layers, n_heads, pf_dim, dropout, dropout_convblock, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device \n",
    "        self.frame = n_frame\n",
    "        self.n_bin = n_bin\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        k = 3 \n",
    "        p = 1\n",
    "\n",
    "        layers_conv_1 = []\n",
    "\n",
    "        ch1 = 48\n",
    "        layers_conv_1.append(layers.Conv2D(ch1, ,)\n",
    "        layers_conv_1.append(layers.\n",
    "        layers_conv_1.append(layers.\n",
    "        layers_conv_1.append(layers.\n",
    "        layers_conv_1.append(layers.\n",
    "        layers_conv_1.append(layers.\n",
    "        layers_conv_1.append(layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, features, label_onset, label_offset, lable_mpe, label_velocity, idx, config, n_slice):\n",
    "        super().__init__()\n",
    "\n",
    "        with open(features, 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "        \n",
    "        with open(label_onset, 'rb') as f:\n",
    "            label_onset = pickle.load(f)\n",
    "        \n",
    "        with open(label_offset, 'rb') as f:\n",
    "            label_offset = pickle.load(f)\n",
    "\n",
    "        with open(label_mpe, 'rb') as f:\n",
    "            label_mpe = pickle.load(f)\n",
    "\n",
    "        if label_velocity is not None:\n",
    "            self.flag_velocity = True\n",
    "            with open(label_velocity, 'rb') as f:\n",
    "                label_velocity = pickle.load(f)\n",
    "        \n",
    "        else:\n",
    "            self.flag_velocity = False\n",
    "        \n",
    "        with open(idx, 'rb') as f:\n",
    "            idx = pickle.load(f)\n",
    "\n",
    "        self.features = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "        self.label_onset = tf.convert_to_tensor(label_onset, dtype=tf.float32)\n",
    "        self.label_offset = tf.convert_to_tensor(label_offset, dtype=tf.float32)\n",
    "        self.label_mpe = tf.convert_to_tensor(label_mpe, dtype=tf.float32)\n",
    "\n",
    "        if self.flag_velocity:\n",
    "            self.label_velocity = tf.convert_to_tensor(label_velocity, dtype=tf.float32)\n",
    "        \n",
    "        if n_slice > 1:\n",
    "            idx_tmp = tf.convert_to_tensor(idx)\n",
    "            self.idx = idx_tmp[:int(len(idx_tmp)/n_slice)*n_slice][::n_slice]\n",
    "        \n",
    "        else:\n",
    "            self.idx = tf.convert_to_tensor(idx)\n",
    "\n",
    "        self.features = config\n",
    "        self.data_size = len(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ixd_features_s = self.idx[idx] - self.config['input']['margin_b']\n",
    "        ixd_features_e = self.idx[idx] + self.config['input']['num_frame'] + self.config['input']['margin_f']\n",
    "\n",
    "        idx_label_s = self.idx[idx]\n",
    "        idx_label_e = self.idx[idx] + self.config['output']['num_frame']\n",
    "\n",
    "        spec = (self.features[ixd_features_s:idx_label_e]).T\n",
    "\n",
    "        leble_onset = self.label_onset[idx_label_s:idx_label_e]\n",
    "\n",
    "        label_offset = self.label_offset[idx_label_s:idx_label_e]\n",
    "\n",
    "        label_mpe = self.label_mpe[idx_label_s:idx_label_e].float()\n",
    "\n",
    "        if self.flag_velocity:\n",
    "            label_velocity = self.label_velocity[idx_label_s:idx_label_e].long()\n",
    "            return spec, leble_onset, label_offset, label_mpe, label_velocity\n",
    "        else:\n",
    "            return spec, leble_onset, label_offset, label_mpe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reminder for later, use dataset = tf.data.Dataset.from_tensor_slices((my_dataset.data, my_dataset.labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
